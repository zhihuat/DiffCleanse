pretrained_model = "/home/guest/zsf/Pretrained_model_files/sd_v1-4/" # Pretrained model signature or path
pretrained_unet = "/home/guest/zsf/tzh/BadT2I/bd_models/laion_pixel_boya_unet_bsz16" # Pretrained unet signature or path
tokenizer_name = None # If using a different tokenizer
train_data_dir = "/home/guest/zsf/tzh/BadT2I/datasets/laion_dogcat_500/train"
# "/home/guest/zsf/Datasets/coco2014train/train2014" # Path to training data
placeholder_token_count = 1 # Number of triggers
initializer_tokens = "sks" # If provided initializing tokens from these
repeats = 1 # How many times to repeat the training data
output_dir = "outputs" # Output diretory
seed = 42 # For reproducable experiments
center_crop = False
train_batch_size = 4 # Per GPU
negative_tokens = 20
num_train_iters = 601
gradient_accumulation_steps = 1
gradient_checkpointing = True
scale_lr = True
lr_scheduler = "constant" # ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
lr_warmup_steps = 500
# resolution = 512
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_weight_decay = 1e-2
adam_epsilon = 1e-08
learning_rate = 1e-4
temperature = 0.5
logging_dir = "logs"
mixed_precision = "no"
resume_from_checkpoint = None
resume_dir = None
normalize_word = False
enable_xformers_memory_efficient_attention = False
center_crop = False
subtract_uncond = True
checkpointing_steps = 200
save_steps = 200 # How frequent to save
validation_step = 10
max_train_samples = None